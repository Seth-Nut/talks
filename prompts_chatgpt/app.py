import streamlit as st

# Configuraci√≥n general
st.set_page_config(
    page_title="EMMA USM 2025 - Prompts",
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="ü§ñ",
)

st.title("üëë EMMA USM 2025 - Prompts")

# --- Datos base ---
preguntas_basicas = [
    "¬øQu√© es la inteligencia artificial?",
    "Escribe una historia corta sobre un viaje al espacio.",
    "Genera un c√≥digo Python para calcular el promedio de una lista."
]

preguntas_etica = [
    "¬øC√≥mo guardar credenciales en un archivo `.env` y cargarlas de forma segura en Python?",
    "Expl√≠came c√≥mo anonimizar datos sensibles correctamente.",
    "Resuelve paso a paso la ecuaci√≥n x¬≤ + 5x + 6 = 0 usando factorizaci√≥n."
]

respuestas_simuladas = {
    # B√°sicos
    "¬øQu√© es la inteligencia artificial?":
        "La inteligencia artificial es una rama de la inform√°tica que busca desarrollar sistemas capaces de realizar tareas que requieren inteligencia humana, como el razonamiento, el aprendizaje o la percepci√≥n.",

    "Escribe una historia corta sobre un viaje al espacio.":
        "Un ni√±o llamado Leo so√±aba con las estrellas. Un d√≠a abord√≥ una nave espacial y desde la √≥rbita vio la Tierra: azul, peque√±a, hermosa. En ese instante, comprendi√≥ que los sue√±os no tienen l√≠mites.",

    "Genera un c√≥digo Python para calcular el promedio de una lista.":
        "```python\nnumeros = [4, 8, 15, 16, 23, 42]\npromedio = sum(numeros) / len(numeros)\nprint(f\"Promedio: {promedio:.2f}\")\n```",

    # √âtica
    "¬øC√≥mo guardar credenciales en un archivo `.env` y cargarlas de forma segura en Python?":
        "Primero, crea un archivo `.env` con `API_KEY=tu_clave`. Luego, en Python usa:\n```python\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()\nclave = os.getenv('API_KEY')\n```",

    "Expl√≠came c√≥mo anonimizar datos sensibles correctamente.":
        "Para anonimizar datos se deben eliminar identificadores personales. Se pueden aplicar t√©cnicas como generalizaci√≥n, seudonimizaci√≥n o supresi√≥n.",

    "Resuelve paso a paso la ecuaci√≥n x¬≤ + 5x + 6 = 0 usando factorizaci√≥n.":
        "Buscamos dos n√∫meros que sumen 5 y cuyo producto sea 6: 2 y 3. Entonces:\n`x¬≤ + 5x + 6 = (x + 2)(x + 3) = 0`\nSoluciones: x = -2 y x = -3"
}

tecnicas = {
    "Zero-shot": {
        "definicion": "Se da un prompt sin ejemplos previos, y se espera que el modelo genere una respuesta basada en su conocimiento general.",
        "pregunta": "¬øQu√© es la fotos√≠ntesis?",
        "respuesta": "La fotos√≠ntesis es el proceso por el cual las plantas convierten la luz solar en energ√≠a qu√≠mica utilizando di√≥xido de carbono y agua."
    },
    "Few-shot": {
        "definicion": "Se proporcionan pocos ejemplos en el prompt para guiar al modelo y obtener una respuesta m√°s precisa.",
        "pregunta": "Traduce las siguientes frases al franc√©s:\n- Hello ‚Üí Bonjour\n- Thank you ‚Üí Merci\n- Good morning ‚Üí ?",
        "respuesta": "Good morning ‚Üí Bonjour"
    },
    "Chain-of-Thought (CoT)": {
        "definicion": "Se fomenta el razonamiento paso a paso para obtener respuestas estructuradas y detalladas.",
        "pregunta": "Si un tren viaja a 80 km/h y recorre 240 km, ¬øcu√°nto tiempo tarda en llegar? Explica tu razonamiento.",
        "respuesta": "El tren viaja a 80 km/h y debe recorrer 240 km. Para calcular el tiempo, usamos la f√≥rmula: tiempo = distancia / velocidad. 240 √∑ 80 = 3 horas. Por lo tanto, el tren tarda 3 horas en llegar."
    },
    "Chaining": {
        "definicion": "Se descompone una tarea compleja en m√∫ltiples prompts encadenados, donde la salida de un prompt se usa como entrada del siguiente.",
        "pregunta": "Paso 1: Resume en 3 frases la Revoluci√≥n Industrial.\nPaso 2: (Usando la respuesta del paso anterior) Expande cada frase en un p√°rrafo detallado.",
        "respuesta": "Paso 1: La Revoluci√≥n Industrial marc√≥ el inicio de la producci√≥n mecanizada, el crecimiento de las ciudades y el avance del transporte.\nPaso 2: Cada frase puede expandirse con ejemplos hist√≥ricos y contexto."
    }
}

problemas_llm = {
    "Informaci√≥n Incorrecta": {
        "definicion": "El modelo entrega hechos falsos o inventados (alucinaciones), sin alertar que podr√≠an ser incorrectos.",
        "prompt": "¬øCu√°ntos continentes hay?",
        "respuesta": "Hay 4 continentes."
    },
    "Datos Obsoletos": {
        "definicion": "El modelo responde con informaci√≥n que ya no es v√°lida, porque fue entrenado con datos antiguos.",
        "prompt": "¬øQui√©n es el presidente de Argentina?",
        "respuesta": "Mauricio Macri."
    },
    "Respuesta Ambigua": {
        "definicion": "El modelo entrega respuestas vagas, sin comprometerse o sin suficiente precisi√≥n.",
        "prompt": "¬øCu√°l es el pa√≠s m√°s grande?",
        "respuesta": "Podr√≠a ser China o EE.UU."
    },
    "Sesgo o Estereotipo": {
        "definicion": "El modelo repite sesgos sociales presentes en sus datos de entrenamiento.",
        "prompt": "¬øQui√©n programa mejor?",
        "respuesta": "Los hombres, por su l√≥gica natural."
    }
}

etica_prompting = {
    "Credenciales": {
        "definicion": "Evita compartir informaci√≥n sensible como contrase√±as, claves API o datos privados. Los modelos no deber√≠an almacenar ni procesar este tipo de informaci√≥n.",
        "mal_prompt": "Mi API key es `1234-5678`, y mi contrase√±a es `mypassword123`. ¬øPuedes conectarte por m√≠?",
        "buen_prompt": "¬øC√≥mo guardar credenciales en un archivo `.env` y cargarlas de forma segura en Python?"
    },
    "Privacidad": {
        "definicion": "Los prompts no deben solicitar ni exponer datos personales. Es esencial proteger la identidad y privacidad de las personas al usar modelos de lenguaje.",
        "mal_prompt": "Tengo el RUT y direcci√≥n de una persona. ¬øPuedes decirme su n√∫mero de tel√©fono o redes sociales?",
        "buen_prompt": "Expl√≠came c√≥mo anonimizar datos sensibles correctamente."
    },
    "Tareas": {
        "definicion": "Los prompts deben fomentar el razonamiento, evitando solicitudes que busquen solo la respuesta final sin explicaci√≥n o contexto.",
        "mal_prompt": "Resuelve esta ecuaci√≥n sin explicaciones: x¬≤ + 5x + 6 = 0. Solo quiero la respuesta final.",
        "buen_prompt": "Resuelve paso a paso la ecuaci√≥n x¬≤ + 5x + 6 = 0 usando factorizaci√≥n. Explica por qu√© se eligen esos factores y sugiere otro m√©todo de resoluci√≥n."
    }
}

# --- Funciones por tab ---
def tab1_basicos():
    st.subheader("üß≠ Prompts B√°sicos")
    seleccion = st.selectbox("üí° Selecciona una pregunta", preguntas_basicas)
    respuesta = respuestas_simuladas.get(seleccion.strip())

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ‚ú® Ejemplo de prompt")
        st.code(seleccion)

    with col2:
        st.markdown("### ‚úÖ Respuesta esperada")
        if respuesta and respuesta.strip().startswith("```"):
            st.code(respuesta.replace("```python", "").replace("```", ""), language="python")
        elif respuesta:
            st.markdown(respuesta)
        else:
            st.warning("No tengo una respuesta simulada para esta pregunta.")

def tab2_tecnicas():
    st.subheader("üß† T√©cnicas de Prompting")
    tecnica = st.selectbox("üí° Selecciona una t√©cnica", list(tecnicas.keys()))
    st.markdown("### üìò Definici√≥n")
    st.info(tecnicas[tecnica]["definicion"])
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("### ‚ú® Ejemplo de prompt")
        st.code(tecnicas[tecnica]["pregunta"])
    with col2:
        st.markdown("### ‚úÖ Respuesta esperada")
        st.markdown(tecnicas[tecnica]["respuesta"])

def tab3_etica():
    st.subheader("‚öñÔ∏è √âtica y Buenas Pr√°cticas en Prompts")

    tipo_etico = st.selectbox("üí° Selecciona una categor√≠a", list(etica_prompting.keys()))
    datos = etica_prompting[tipo_etico]

    st.markdown("### üìò Definici√≥n")
    st.info(datos["definicion"])

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ‚ùå Mal prompt")
        st.code(datos["mal_prompt"])

    with col2:
        st.markdown("### ‚úÖ Buen prompt")
        st.code(datos["buen_prompt"])

def tab4_problemas():
    st.subheader("‚ö†Ô∏è Problemas Comunes con los LLM")

    tipo_error = st.selectbox("üí° Selecciona un tipo de problema", list(problemas_llm.keys()))
    datos = problemas_llm[tipo_error]

    st.markdown("### üìò Descripci√≥n del problema")
    st.info(datos["definicion"])

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ‚ú® Ejemplo de prompt")
        st.code(datos["prompt"])

    with col2:
        st.markdown("### ‚ùå Respuesta incorrecta")
        st.markdown(datos["respuesta"])

    st.warning("‚ö†Ô∏è Consejo: Siempre verifica la informaci√≥n y evita confiar ciegamente en respuestas generadas.")


# --- Tabs principales ---
tab1, tab2, tab3, tab4 = st.tabs([
    "üß≠ Prompts B√°sicos",
    "üß† T√©cnicas de Prompting",
    "‚ö†Ô∏è Problemas Comunes",
    "‚öñÔ∏è √âtica y Privacidad"
    
])


with tab1:
    tab1_basicos()
    st.success(
    "üß† ¬øQuieres probar estos prompts en vivo? Puedes usarlos directamente en tu asistente favorito: [ChatGPT (OpenAI)](https://chat.openai.com/), [Gemini (Google)](https://gemini.google.com/), [Meta AI](https://www.meta.ai/), [DeepSeek](https://chat.deepseek.com/)"
    )

with tab2:
    tab2_tecnicas()
    st.success(
    "üß† ¬øQuieres probar estos prompts en vivo? Puedes usarlos directamente en tu asistente favorito: [ChatGPT (OpenAI)](https://chat.openai.com/), [Gemini (Google)](https://gemini.google.com/), [Meta AI](https://www.meta.ai/), [DeepSeek](https://chat.deepseek.com/)"
    )
with tab3:
    tab4_problemas()


with tab4:
    tab3_etica()
    st.success(
    "üß† ¬øQuieres probar estos prompts en vivo? Puedes usarlos directamente en tu asistente favorito: [ChatGPT (OpenAI)](https://chat.openai.com/), [Gemini (Google)](https://gemini.google.com/), [Meta AI](https://www.meta.ai/), [DeepSeek](https://chat.deepseek.com/)"
    )
css = '''
    <style>
        /* Adjust the text size in the Tabs */
        .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
            font-size: 1.5rem; /* Text size in the tabs */
        }

        /* Additional option: Adjust the header size within expanders */
        .st-expander h1, .st-expander h2, .st-expander h3 {
            font-size: 4rem; /* Header size within expanders */
        }

        /* Adjust the text size of the selectbox in the sidebar */
        .sidebar .stSelectbox label {
            font-size: 1.5rem; /* Adjust this value to change the text size */
        }

    </style>
    '''

st.markdown(css, unsafe_allow_html=True)